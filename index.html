<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
  <head>
  <meta name="google-site-verification" content="xDNWUvx6Q5EWK5YYSyKvK8DZTmvXhKsGX203Ll-BFFE" >	
  <meta name=viewport content=“width=800”>
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
  <style type="text/css">
  @import url(https://fonts.googleapis.com/css?family=Roboto:400,400italic,500,500italic,700,700italic,900,900italic,300italic,300);
  /* @import url(https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons); */
    /* Color scheme stolen from Sergey Karayev */
    a {
    /*color: #b60a1c;*/
    color: #1772d0;
    /*color: #bd0a36;*/
    text-decoration:none;
    }
    a:focus, a:hover {
    color: #f09228;
    text-decoration:none;
    }
    body,td,th,tr,p,a {
    font-family: 'Roboto', sans-serif;
    font-size: 15px;
    font-weight: 300;
    }
    strong {
    font-family: 'Roboto', sans-serif;
    /*font-family: 'Lato', Verdana, Helvetica, sans-serif;*/
    /*font-family: 'Avenir Next';*/
    font-size: 15px;
    font-weight: 400;
    }
    heading {
    /*font-family: 'Lato', Verdana, Helvetica, sans-serif;*/
    font-family: 'Roboto', sans-serif;
    /*font-family: 'Avenir Next';*/
    /*src: url("./fonts/Roboto_Mono_for_Powerline.ttf");*/
    font-size: 24px;
    font-weight: 400;
    }
    papertitle {
    /*font-family: 'Lato', Verdana, Helvetica, sans-serif;*/
    font-family: 'Roboto', sans-serif;
    /*font-family: 'Avenir Next';*/
    /*src: url("./fonts/Roboto_Mono_for_Powerline.ttf");*/
    font-size: 15px;
    font-weight:500;
    }
    name {
    /*font-family: 'Lato', Verdana, Helvetica, sans-serif;*/
    font-family: 'Roboto', sans-serif;
    /*font-family: 'Avenir Next';*/
    font-weight: 400;
    font-size: 32px;
    }
    .one
    {
    width: 160px;
    height: 140px;
    position: relative;
    }
    .two
    {
    width: 160px;
    height: 160px;
    position: absolute;
    transition: opacity .2s ease-in-out;
    -moz-transition: opacity .2s ease-in-out;
    -webkit-transition: opacity .2s ease-in-out;
    }
    .fade {
     transition: opacity .2s ease-in-out;
     -moz-transition: opacity .2s ease-in-out;
     -webkit-transition: opacity .2s ease-in-out;
    }
    span.highlight {
        background-color: #ffffd0;
    }
  </style>
  <link rel="icon" type="image/png" href="img/star.png">
  <title>Xingyu Chen - Homepage</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
  <script src="script/functions.js"></script>
  </head>
  <body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
    <td>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <!--<tr onmouseout="headshot_stop()" onmouseover="headshot_start()">-->
      <tr>
        <td width="67%" valign="middle">
        <p align="center">
          <name>Xingyu Chen (陈星宇)</name>
        </p>
        <p>
          I am passionate about bridging the physical and information worlds, and advancing next-generation AR/robotics with 3D spatial intelligence.

          I obtained my M.S. and B.S. from the <a href="http://www.aiar.xjtu.edu.cn/#">Institute of Artificial Intelligence and Robotics</a> at 
          <a href="http://en.xjtu.edu.cn">Xi’an Jiaotong University</a> and <a href="http://english.cqu.edu.cn/">Chongqing University</a>.
        </p>  
        <p>
          In 2021 and 2022, I was a research intern at <a href="https://ai.tencent.com/ailab/en/index/">Tencent AI lab</a>, 
          working with <a href="https://xuanwangvc.github.io/">Xuan Wang</a> and <a href="https://qzhang-cv.github.io/">Qi Zhang</a>,
          hosted by <a href="https://juewang725.github.io/">Jue Wang</a>.
        </p>
          <!-- My research topics include computer vision, graphics, and neural fields. 
          Much of my research is about inferring the physical world (shape, appearance, motion, etc.) and camera poses from images.  -->
          My research topics include computer vision, graphics, and neural fields, 
          specifically in inferring the physical world (shape, appearance, motion, etc.) and camera poses from images. 
          <!-- I focus on computer vision, graphics, and neural fields to infer the physical world (shape, appearance, motion, etc.) and camera poses from images.   -->
        <p>
        </p>
        <p align=center>
          <a href="mailto:xingyu@stu.xjtu.edu.cn">Email</a> &nbsp|&nbsp
          <a href="https://scholar.google.com/citations?user=gDHPrWEAAAAJ&hl=zh-CN">Google Scholar</a> &nbsp|&nbsp
          <a href="https://github.com/rover-xingyu/">GitHub</a> 

        </p>
        </td>
        <td width="33%">
          <img src="img/me3.jpg" width="250" alt="headshot">
        </td>
      </tr>
      </table>

      <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="10%" valign="middle">
          <a href="https://ethz.ch/en.html"><img src="media/eth_logo.png" width="120"></a>
        </td>
        <td width="10%" valign="middle">
          <a href="https://www.is.mpg.de/"><img src="media/mpi_logo.png" width="60"></a>
        </td>
        <td width="10%" valign="middle">
          <a href="https://research.google/"><img src="media/google_logo.png" width="55"></a>
        </td>
        <td width="10%" valign="middle">
          <a href="https://about.facebook.com/realitylabs/"><img src="media/frl_logo.png" width="60"></a>
        </td>
        <td width="10%" valign="middle">
          <a href="https://www.inria.fr/en/"><img src="media/inria_logo.jpg" width="90"></a>
        </td>
        <td width="10%" valign="middle">
          <a href="https://vision.in.tum.de"><img src="media/tum_logo.jpg" width="60"></a>
        </td>	
        <td width="10%" valign="middle">
          <a href="https://www.vibot.org"><img src="media/vibot_logo_transparent.png" width="35"></a></td>
        </td>
        <td width="10%" valign="middle">
          <a href="https://en.xjtu.edu.cn/"><img src="media/xjtu_logo.png" width="50"></a></td>
        </td>	
      </tr>
      </table> -->

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td>
          <heading>News</heading>
            <ul>
              <!-- <li><strong>03/2023</strong> <span style="color:#ff0000;">I am actively looking for a Ph.D. position. Please <a href="mailto:xingyu@stu.xjtu.edu.cn">Email</a> me if there are potential opportunities!</span></li> -->
              <li><strong>02/2023</strong> 2 papers (<a href="https://rover-xingyu.github.io/L2G-NeRF/">L2G-NeRF</a>, <a href="https://fanegg.github.io/UV-Volumes/">UV Volumes</a>) accepted to <strong>CVPR 2023</strong>! </li>
              <li><strong>01/2023</strong> 1 paper (<a href="https://arxiv.org/pdf/2211.11439.pdf">PROCA</a>) accepted to <strong>ICRA 2023</strong>! </li>
              <li><strong>09/2022</strong> 1 paper (<a href="https://arxiv.org/pdf/2210.04543.pdf">Learning 2D-3D Point-Line Correspondences</a>) accepted to <strong>RA-L 2022</strong>! </li>
              <li><strong>03/2022</strong> 1 paper (<a href="https://rover-xingyu.github.io/Ha-NeRF/">Ha-NeRF</a>) accepted to <strong>CVPR 2022</strong>! </li>
              <li><strong>04/2020</strong> 1 paper (<a href="https://arxiv.org/pdf/2210.04562.pdf">RDS-SLAM</a>) accepted to <strong>IV 2020</strong>! </li>
              <li><strong>01/2020</strong> 1 paper (<a href="https://ieeexplore.ieee.org/document/9196609">NCM</a>) accepted to <strong>ICRA 2020</strong>! </li>
              <!-- <a href="javascript:toggleblock(&#39;old_news&#39;)">---- show more ----</a>
              <div id="old_news" style="display: none;">
              <li><strong>08/2021</strong> Join <a href="https://research.fb.com/category/augmented-reality-virtual-reality/">Facebook Reality Labs (FRL)</a> as a research intern this fall.</li> -->
            </div></div>
            </ul>
        </td>
      </tr>
      <tr>
        <td width="100%" valign="middle">
          <heading>Research</heading>
          <p>
            <!-- Representative papers are <span class="highlight">highlighted</span>. -->
            <!-- * denotes equal contribution co-authorship, † denotes corresponding author -->
            Equal Contribution * , Corresponding Author †
          </p>
        </td>
      </tr>
      </table>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20" >
        <tr onmouseout="l2g_stop()" onmouseover="l2g_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <div class="two" id='l2g_image'><video  width=100% height=100% muted autoplay loop>
              <source src="img/l2g.mp4" type="video/mp4">
              Your browser does not support the video tag.
              </video></div>
              <img src='img/l2g.jpg' width="160">
            </div>
            <script type="text/javascript">
              function l2g_start() {
                document.getElementById('l2g_image').style.opacity = "1";
              }
        
              function l2g_stop() {
                document.getElementById('l2g_image').style.opacity = "0";
              }
              l2g_stop()
            </script>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://rover-xingyu.github.io/L2G-NeRF/">
              <papertitle>L2G-NeRF: Local-to-Global Registration for Bundle-Adjusting Neural Radiance Fields</papertitle>
            </a>
            <br>
            <a href="https://fanegg.github.io/">Yue Chen*</a>,
            <strong>Xingyu Chen*</strong>,
            <a href="https://xuanwangvc.github.io/">Xuan Wang†</a>,
            <a href="https://qzhang-cv.github.io/">Qi Zhang</a>,
            <a href="https://yuguo-xjtu.github.io/">Yu Guo†</a>,
            <a href="https://scholar.google.com/citations?user=4oXBp9UAAAAJ&hl=en">Ying Shan</a>,
            <a href="http://www.aiar.xjtu.edu.cn/info/1046/1242.htm">Fei Wang</a>
            <br>
            IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2023
            <br>
            <a href="https://rover-xingyu.github.io/L2G-NeRF/">project page</a> /
            <a href="https://arxiv.org/pdf/2211.11505.pdf">arXiv</a> /
            <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Local-to-Global_Registration_for_Bundle-Adjusting_Neural_Radiance_Fields_CVPR_2023_paper.pdf">paper</a> /
            <a href="https://github.com/rover-xingyu/L2G-NeRF">code</a> /
            <a href="https://openaccess.thecvf.com/content/CVPR2023/supplemental/Chen_Local-to-Global_Registration_for_CVPR_2023_supplemental.pdf">supplementary</a> /
            <a href="https://youtu.be/y8XP9Umt6Mw">video</a> /
            <a href="https://rover-xingyu.github.io/L2G-NeRF/files/l2gnerf_poster.pdf">poster</a>
            <p></p>
            <p>
              We combine local and global alignment via differentiable parameter estimation solvers to achieve robust bundle-adjusting Neural Radiance Fields. 
            </p>
          </td>
        </tr> 

          <tr onmouseout="uv_stop()" onmouseover="uv_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='uv_image'><video  width=100% height=100% muted autoplay loop>
                <source src="img/uv_volume.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='img/retexture.png' width="160">
              </div>
              <script type="text/javascript">
                function uv_start() {
                  document.getElementById('uv_image').style.opacity = "1";
                }
          
                function uv_stop() {
                  document.getElementById('uv_image').style.opacity = "0";
                }
                uv_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://fanegg.github.io/UV-Volumes/">
                <papertitle>UV Volumes for Real-time Rendering of Editable Free-view Human Performance</papertitle>
              </a>
              <br>
              <a href="https://fanegg.github.io/">Yue Chen*</a>,
              <a href="https://xuanwangvc.github.io/">Xuan Wang*</a>,
              <strong>Xingyu Chen</strong>,
              <a href="https://qzhang-cv.github.io/">Qi Zhang</a>,
              <a href="https://xiaoyu258.github.io/">Xiaoyu Li</a>,
              <a href="https://yuguo-xjtu.github.io/">Yu Guo†</a>,
              <a href="https://juewang725.github.io/">Jue Wang</a>,
              <a href="http://www.aiar.xjtu.edu.cn/info/1046/1242.htm">Fei Wang</a>
              <br>
              IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2023
              <br>
              <a href="https://fanegg.github.io/UV-Volumes/">project page</a> /
              <a href="https://arxiv.org/pdf/2203.14402.pdf">arXiv</a> /
              <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_UV_Volumes_for_Real-Time_Rendering_of_Editable_Free-View_Human_Performance_CVPR_2023_paper.pdf">paper</a> /
              <a href="https://github.com/fanegg/UV-Volumes">code /
              <a href="https://openaccess.thecvf.com/content/CVPR2023/supplemental/Chen_UV_Volumes_for_CVPR_2023_supplemental.pdf">supplementary</a> /
              <a href="https://youtu.be/JftQnXLMmPc">video</a></a> /
              <a href="https://fanegg.github.io/UV-Volumes/files/uvvolumes_poster.pdf">poster</a>
              <p></p>
              <p> We separate high-frequency human appearance from 3D volume and encode them into a 2D texture, which enables real-time rendering and retexturing.
              </p>
            </td>
          </tr> 
      
        <tr onmouseout="PROCA_stop()" onmouseover="PROCA_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <div class="two" id='PROCA_image'>
                <img src='img/proca2.jpg' width="160">
              </div>
              <img src='img/proca1.jpg' width="160">
            </div>
            <script type="text/javascript">
              function PROCA_start() {
                document.getElementById('PROCA_image').style.opacity = "1";
              }
        
              function PROCA_stop() {
                document.getElementById('PROCA_image').style.opacity = "0";
              }
              PROCA_stop()
            </script>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://arxiv.org/pdf/2211.11439.pdf">
              <papertitle>PROCA: Place Recognition under Occlusion and Changing Appearance via Disentangled Representations</papertitle>
            </a>
            <br>
            <a href="https://fanegg.github.io/">Yue Chen</a>,
            <strong>Xingyu Chen†</strong>,
            <a href="https://github.com/YicenJoJo">Yicen Li</a>
            <br>
            IEEE International Conference on Robotics and Automation (<strong>ICRA</strong>), 2023
            <br>
            <a href="https://arxiv.org/pdf/2211.11439.pdf">arXiv</a> /
            <a href="https://github.com/rover-xingyu/PROCA">code</a> /
            <a href="https://www.youtube.com/watch?v=W_tol4aHIQk">video</a> /
            <a href="https://fanegg.github.io/PROCA/PROCA_poster.pdf">poster</a>
            <p></p>
            <p>
              We decompose the image representation into place, appearance, and occlusion code and use the place code as a descriptor to retrieve images.
            </p>
          </td>
        </tr>
    
        <tr onmouseout="bpnpl_stop()" onmouseover="bpnpl_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <div class="two" id='bpnpl_image'><video  width=100% height=100% muted autoplay loop>
              <source src="img/bpnpl.mp4" type="video/mp4">
              Your browser does not support the video tag.
              </video></div>
              <img src='img/bpnpl.png' width="160">
            </div>
            <script type="text/javascript">
              function bpnpl_start() {
                document.getElementById('bpnpl_image').style.opacity = "1";
              }

              function bpnpl_stop() {
                document.getElementById('bpnpl_image').style.opacity = "0";
              }
              bpnpl_stop()
            </script>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://arxiv.org/pdf/2210.04543.pdf">
              <papertitle>Sparse Semantic Map-Based Monocular Localization in Traffic Scenes Using Learned 2D-3D Point-Line Correspondences</papertitle>
            </a>
            <br>
            <strong>Xingyu Chen</strong>,
            <a href="https://gr.xjtu.edu.cn/en/web/jrxue">Jianru Xue†</a>,
            <a href="https://gr.xjtu.edu.cn/en/web/pangsm/4">Shanmin Pang</a>
            <br>
            IEEE Robotics and Automation Letters (<strong>RA-L</strong>), 2022
            <br>
            <a href="https://arxiv.org/pdf/2210.04543.pdf">arXiv</a> /
            <a href="https://ieeexplore.ieee.org/document/9895296">paper</a> 
            <p></p>
            <p>
              Given a sparse semantic map (e.g., pole lines, traffic sign midpoints), we estimate camera poses by learning 2D-3D point-line correspondences.
            </p>
          </td>
        </tr> 
    
        <tr onmouseout="hanerf_stop()" onmouseover="hanerf_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <div class="two" id='hanerf_image'><video  width=100% height=100% muted autoplay loop>
              <source src="img/video-teaser_crop.mp4" type="video/mp4">
              Your browser does not support the video tag.
              </video></div>
              <img src='img/teaser.png' width="160">
            </div>
            <script type="text/javascript">
              function hanerf_start() {
                document.getElementById('hanerf_image').style.opacity = "1";
              }

              function hanerf_stop() {
                document.getElementById('hanerf_image').style.opacity = "0";
              }
              hanerf_stop()
            </script>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://rover-xingyu.github.io/Ha-NeRF/">
              <papertitle>Ha-NeRF&#x1F606: Hallucinated Neural Radiance Fields in the Wild</papertitle>
            </a>
            <br>
            <strong>Xingyu Chen</strong>,
            <a href="https://qzhang-cv.github.io/">Qi Zhang†</a>,
            <a href="https://xiaoyu258.github.io/">Xiaoyu Li</a>,
            <a href="https://fanegg.github.io/">Yue Chen</a>,
            <a href="https://rover-xingyu.github.io/">Ying Feng</a>,
            <a href="https://xuanwangvc.github.io/">Xuan Wang</a>,
            <a href="https://juewang725.github.io/">Jue Wang</a>
            <br>
            IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2022
            <br>
            <a href="https://rover-xingyu.github.io/Ha-NeRF/">project page</a> /
            <a href="https://arxiv.org/pdf/2111.15246.pdf">arXiv</a> /
            <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Chen_Hallucinated_Neural_Radiance_Fields_in_the_Wild_CVPR_2022_paper.pdf">paper</a> /
            <a href="https://rover-xingyu.github.io/Ha-NeRF/files/Ha_NeRF_CVPR_2022_supp.pdf">supplementary</a> /
            <a href="https://github.com/rover-xingyu/Ha-NeRF">code</a> /
            <a href="https://www.youtube.com/watch?v=Qrz0gfcTdQs">video</a> /
            <a href="https://rover-xingyu.github.io/Ha-NeRF/files/Ha_NeRF_poster.pdf">poster</a>
            <p></p>
            <p>
              We recover NeRF from tourism images with variable appearance and occlusions, 
              consistently rendering free-occlusion views with hallucinated appearances.
            </p>
          </td>
        </tr>

        <!-- rdsslam -->
        <tr onmouseout="rds_stop()" onmouseover="rds_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <div class="two" id='rds_image'><video  width=100% height=100% muted autoplay loop>
              <source src="img/rds-slam_cccc.mp4" type="video/mp4">
              Your browser does not support the video tag.
              </video></div>
              <img src='img/rds-slam2.png' width="160">
            </div>
            <script type="text/javascript">
              function rds_start() {
                document.getElementById('rds_image').style.opacity = "1";
              }

              function rds_stop() {
                document.getElementById('rds_image').style.opacity = "0";
              }
              rds_stop()
            </script>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://arxiv.org/pdf/2210.04562.pdf">
              <papertitle>Using Detection, Tracking and Prediction in Visual SLAM to Achieve Real-time Semantic Mapping of Dynamic Scenarios</papertitle>
            </a>
            <br>
            <strong>Xingyu Chen</strong>,
            <a href="https://gr.xjtu.edu.cn/en/web/jrxue">Jianru Xue†</a>,
            <a href="http://www.lotvs.net/JianwuFang/">Jianwu Fang</a>,
            <a href="https://rover-xingyu.github.io/">Yuxin Pan</a>,
            <a href="https://scholar.google.com.hk/citations?user=iqMe3p8AAAAJ&hl=zh-CN">Nanning Zheng</a>
            <br>
            IEEE Intelligent Vehicles Symposium (<strong>IV</strong>), 2020
            <br>
            <a href="https://arxiv.org/pdf/2210.04562.pdf">arXiv</a> /
            <a href="https://ieeexplore.ieee.org/document/9304693">paper</a>
            <p></p>
            <p>
              Instead of detecting objects in all frames, 
              we detect only keyframes and embed an efficient prediction mechanism in all incoming frames.  
            </p>
          </td>
        </tr>     

        <!-- NCM -->
        <tr onmouseout="NCM_stop()" onmouseover="NCM_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <div class="two" id='NCM_image'><video  width=100% height=100% muted autoplay loop>
              <source src="img/ICRA2020_web_c.mp4" type="video/mp4">
              Your browser does not support the video tag.
              </video></div>
              <img src='img/ICRA2020_overview.png' width="160">
            </div>
            <script type="text/javascript">
              function NCM_start() {
                document.getElementById('NCM_image').style.opacity = "1";
              }

              function NCM_stop() {
                document.getElementById('NCM_image').style.opacity = "0";
              }
              NCM_stop()
            </script>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://www.researchgate.net/profile/Pan-Yuxin/publication/340082526_Navigation_Command_Matching_for_Vision-based_Autonomous_Driving/links/600a288192851c13fe2a8d5a/Navigation-Command-Matching-for-Vision-based-Autonomous-Driving.pdf">
              <papertitle>Navigation Command Matching for Vision-based Autonomous Driving</papertitle>
            </a>
            <br>
            <a href="https://rover-xingyu.github.io/">Yuxin Pan</a>,
            <a href="https://gr.xjtu.edu.cn/en/web/jrxue">Jianru Xue†</a>,
            <a href="https://scholar.google.no/citations?user=n4v0ev8AAAAJ&hl=no">Pengfei Zhang</a>,
            <a href="https://wlouyang.github.io/">Wanli Ouyang</a>,
            <a href="http://www.lotvs.net/JianwuFang/">Jianwu Fang</a>,
            <strong>Xingyu Chen</strong>
            <br>
            IEEE International Conference on Robotics and Automation (<strong>ICRA</strong>), 2020
            <br>
            <a href="https://www.researchgate.net/profile/Pan-Yuxin/publication/340082526_Navigation_Command_Matching_for_Vision-based_Autonomous_Driving/links/600a288192851c13fe2a8d5a/Navigation-Command-Matching-for-Vision-based-Autonomous-Driving.pdf">ResearchGate</a> /
            <a href="https://ieeexplore.ieee.org/document/9196609">paper</a>
            <p></p>
            <p>We propose a navigation command matching model to discriminate actions generated from sub-optimal policies via smooth rewards.</p>
          </td>
        </tr>
      </table>
  
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20" >
        <br>
        <tr>
          <td>
            <heading>Projects</heading>
          </td>
        </tr>
      <!-- kuafu -->
        <tr onmouseout="kuafu_stop()" onmouseover="kuafu_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <div class="two" id='kuafu_image'><video  width=100% height=100% muted autoplay loop>
              <source src="img/kuafu.mp4" type="video/mp4">
              Your browser does not support the video tag.
              </video></div>
              <img src='img/kuafu.png' width="160">
            </div>
            <script type="text/javascript">
              function kuafu_start() {
                document.getElementById('kuafu_image').style.opacity = "1";
              }

              function kuafu_stop() {
                document.getElementById('kuafu_image').style.opacity = "0";
              }
              kuafu_stop()
            </script>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://gr.xjtu.edu.cn/en/web/jrxue/home">
              <papertitle>Kuafu (Autonomous Vehicle)</papertitle>
            </a>
            <br>
            <strong>IEEE Intelligent Transportation Systems Institute Lead Award</strong>
            <br>
            Second Place for Navigating in GPS-denied Environments, Intelligent Vehicle Future Challenge, 2019
            <br>
            <p></p>
            <p>Lidar Odometry, Mapping, and Localization.</p>
          </td>
        </tr> 

      <!-- robot_hand -->
        <tr onmouseout="robot_hand_stop()" onmouseover="robot_hand_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <div class="two" id='robot_hand_image'><video  width=100% height=100% muted autoplay loop>
              <source src="img/robot_hand.mp4" type="video/mp4">
              Your browser does not support the video tag.
              </video></div>
              <img src='img/robot_hand.png' width="160">
            </div>
            <script type="text/javascript">
              function robot_hand_start() {
                document.getElementById('robot_hand_image').style.opacity = "1";
              }

              function robot_hand_stop() {
                document.getElementById('robot_hand_image').style.opacity = "0";
              }
              robot_hand_stop()
            </script>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://rover-xingyu.github.io/">
              <papertitle>Robotic Hand</papertitle>
            </a>
            <br>
            <strong>Xingyu Chen</strong>,
            Zhili Liu,
            Yi Liu,
            Yang Li,
            Chenyang Liu
            <br>
            National Second Prize, The 11th Chinese College Students Computer Design Contest, 2018
            <br>
            <p></p>
            <p>Sensor fusion of IMU and BLE for localization.
            <br>Hand gesture recognition based on computer vision.
            <br>Robot hand controller based on potentiometer.
            </p>
          </td>
        </tr>  

      <!-- robot_arm -->
        <tr onmouseout="robot_arm_stop()" onmouseover="robot_arm_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <div class="two" id='robot_arm_image'><video  width=100% height=100% muted autoplay loop>
              <source src="img/robot_arm.mp4" type="video/mp4">
              Your browser does not support the video tag.
              </video></div>
              <img src='img/robot_arm.png' width="160">
            </div>
            <script type="text/javascript">
              function robot_arm_start() {
                document.getElementById('robot_arm_image').style.opacity = "1";
              }

              function robot_arm_stop() {
                document.getElementById('robot_arm_image').style.opacity = "0";
              }
              robot_arm_stop()
            </script>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://rover-xingyu.github.io/">
              <papertitle>Robotic Arm</papertitle>
            </a>
            <br>
            <strong>Xingyu Chen</strong>,
            Zhili Liu,
            Yang Li
            <br>
            National Second Prize, The 11th iCAN International Contest of innovAtioN, 2017
            <br>
            <p></p>
            <p>Hand gesture recognition based on computer vision.
            <br>Structure design based on four-bar linkage.
            </p>
          </td>
        </tr>        
      </table>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <br>
        <tr>
          <td>
            <heading>Invited Talks</heading>
          </td>
        </tr>
        <!-- HaNeRF_talk -->
        <tr onmouseout="HaNeRF_talk_stop()" onmouseover="HaNeRF_talk_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <div class="two" id='HaNeRF_talk_image'><video  width=100% height=100% muted autoplay loop>
              <img src='img/HaNeRF_talk.png' width="160">
              Your browser does not support the video tag.
              </video></div>
              <img src='img/HaNeRF_talk.png' width="160">
            </div>
            <script type="text/javascript">
              function HaNeRF_talk_start() {
                document.getElementById('HaNeRF_talk_image').style.opacity = "1";
              }

              function HaNeRF_talk_stop() {
                document.getElementById('HaNeRF_talk_image').style.opacity = "0";
              }
              HaNeRF_talk_stop()
            </script>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://www.shenlanxueyuan.com/open/course/155">
              <papertitle>光影幻象：神经辐射场中的时空流转</papertitle>
            </a>
            <br>
            <strong>Xingyu Chen</strong>
            <br>
            <em>Shenlan College online education</em>, 2022
            <br>
            <p></p>
            <p>Introduction about Neural Radiance Fields (NeRF) for unconstrained photo collections.
            <br>Including <a href="https://www.matthewtancik.com/nerf">NeRF</a>, <a href="https://nerf-w.github.io/">NeRF in the Wild</a>, and <a href="https://rover-xingyu.github.io/Ha-NeRF/">Ha-NeRF</a> 
            </p>
          </td>
        </tr> 
      </table>

      <!-- Academic Services -->
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tr>
          <td>
            <heading>Academic Services</heading>
              <ul>
                <li><strong>Conference Reviewer</strong>: CVPR, ICCV, NeurIPS
              </ul>
          </td>
        </tr>
        </table>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td>
        <br>
        <p align="right">
          <font size="2">
          template adapted from <a href="https://jonbarron.info/"><font size="2">this awesome website</font></a>
          <br>
          Last updated: Mar 2023
        </font>
        </p>
        </td>
      </tr>
      </table>
      <script type="text/javascript">
      var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
          document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));

      </script> <script type="text/javascript">
      try {
          var pageTracker = _gat._getTracker("UA-116734954-1");
          pageTracker._trackPageview();
          } catch(err) {}
      </script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-116734954-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-116734954-1');
</script>
    </td>
    </tr>
  </table>
  </body>
</html>
<!--  -->
